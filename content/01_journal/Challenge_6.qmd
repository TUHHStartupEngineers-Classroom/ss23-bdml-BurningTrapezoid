---
title: "Challenge 6: Deep Learning"
author: "Andreas Jessen"
---

```{r}
#| message: false
# Libraries
library(tidyverse)
library(lime)
library(recipes)
library(rsample)
library(yardstick)
library(corrr)
library(tensorflow)
library(keras)
```

```{r}
#| message: false
churn_data_raw <- read_csv("Data/WA_Fn-UseC_-Telco-Customer-Churn.csv")

glimpse(churn_data_raw)
```

```{r}
churn_data_tbl <- churn_data_raw %>%
                  select(-customerID) %>%
                  drop_na(TotalCharges) %>%
                  select(Churn, everything())

```

```{r}
# Split test/training sets
set.seed(100)
train_test_split <- initial_split(churn_data_tbl, prop = 0.8)
train_test_split

## <Analysis/Assess/Total>
## <5626/1406/7032>

# Retrieve train and test sets
train_tbl <- training(train_test_split)
test_tbl  <- testing(train_test_split)

```

```{r}
# Create recipe
rec_obj <- recipe(Churn ~ ., data = train_tbl) %>%
    step_rm(Churn) %>% 
    step_discretize(tenure, options = list(cuts = 6)) %>%
    step_log(TotalCharges) %>%
    step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %>%
    step_center(all_predictors(), -all_outcomes()) %>%
    step_scale(all_predictors(), -all_outcomes()) %>%
    prep(data = train_tbl)
```

```{r}
# Predictors
x_train_tbl <- bake( rec_obj , new_data = train_tbl )
x_test_tbl  <- bake( rec_obj , new_data = test_tbl )
```

```{r}
# Response variables for training and testing sets
y_train_vec <- ifelse( train_tbl$Churn == "Yes", 1, 0 )
y_test_vec  <- ifelse( test_tbl$Churn == "Yes", 1, 0 )
```

```{r}
#| message: false
# Building our Artificial Neural Network
model_keras <- keras_model_sequential()

model_keras %>% 
    # First hidden layer
    layer_dense(
        units              = 16, 
        kernel_initializer = "uniform", 
        activation         = "relu", 
        input_shape        = ncol(x_train_tbl)) %>% 
    # Dropout to prevent overfitting
    layer_dropout(rate = 0.1) %>%
    # Second hidden layer
    layer_dense(
        units              = 16, 
        kernel_initializer = "uniform", 
        activation         = "relu") %>% 
    # Dropout to prevent overfitting
    layer_dropout(rate = 0.1) %>%
    # Output layer
    layer_dense(
        units              = 1, 
        kernel_initializer = "uniform", 
        activation         = "sigmoid") %>% 
    # Compile ANN
    compile(
        optimizer = 'adam',
        loss      = 'binary_crossentropy',
        metrics   = c('accuracy')
    )
model_keras
```

```{r}
# Fit the keras model to the training data
fit_keras <-  model_keras %>% fit(
    x               = as.matrix(x_train_tbl) , 
    y               = as.matrix(y_train_vec) , 
    epochs          = 35 ,
    batch_size      = 50 , 
    validation_data = list(as.matrix(x_test_tbl), as.matrix(y_test_vec)) ,
    validation_split= 0.3 
    )
fit_keras
```

```{r}
# Plot the training/validation history of our Keras model
plot(fit_keras) +
  labs(title = "Deep Learning Training Results") +
  theme_minimal() +
  theme(legend.position  = "bottom", 
        strip.placement  = "inside")
```

```{r}
# # Predicted Class
# yhat_keras_class_vec <- predict_classes(object = model_keras, x = as.matrix(x_test_tbl)) %>%
#     as.vector()
# 
# # Predicted Class Probability
# yhat_keras_prob_vec  <- predict_proba(object = model_keras, x = as.matrix(x_test_tbl)) %>%
#     as.vector()
```


predict_classes() and predict_proba() seem to be deprecated. As this Challenge is optional and the following steps seem to be a repetition of the last Chapter I decided to not continue further.
